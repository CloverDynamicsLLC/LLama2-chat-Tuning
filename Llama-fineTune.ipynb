{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afec07e3-9c1f-407f-a0e8-fafb0170d4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch==2.2.0 torchvision==0.17.0 torchaudio==2.2.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git@v4.38.2\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q trl xformers wandb datasets einops gradio sentencepiece bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e16436-ddb9-4252-9f67-2ad98216b09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging, TextStreamer\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb, platform, gradio, warnings\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86efb1cc-fde9-4c4d-8d90-046a115973c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_system_specs():\n",
    "    # Check if CUDA is available\n",
    "    is_cuda_available = torch.cuda.is_available()\n",
    "    print(\"CUDA Available:\", is_cuda_available)\n",
    "# Get the number of available CUDA devices\n",
    "    num_cuda_devices = torch.cuda.device_count()\n",
    "    print(\"Number of CUDA devices:\", num_cuda_devices)\n",
    "    if is_cuda_available:\n",
    "        for i in range(num_cuda_devices):\n",
    "            # Get CUDA device properties\n",
    "            device = torch.device('cuda', i)\n",
    "            print(f\"--- CUDA Device {i} ---\")\n",
    "            print(\"Name:\", torch.cuda.get_device_name(i))\n",
    "            print(\"Compute Capability:\", torch.cuda.get_device_capability(i))\n",
    "            print(\"Total Memory:\", torch.cuda.get_device_properties(i).total_memory, \"bytes\")\n",
    "    # Get CPU information\n",
    "    print(\"--- CPU Information ---\")\n",
    "    print(\"Processor:\", platform.processor())\n",
    "    print(\"System:\", platform.system(), platform.release())\n",
    "    print(\"Python Version:\", platform.python_version())\n",
    "print_system_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c14536-0ec9-4a91-b829-db79d7c6574c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pre trained model\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "\n",
    "# Dataset name\n",
    "dataset_dir = \"cornell movie-dialogs corpus\"\n",
    "\n",
    "# Hugging face repository link to save fine-tuned model(Create new repository in huggingface,copy and paste here)\n",
    "new_model = \"llama-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbe100-99b0-44eb-8a40-de78e2250560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bef3b1-2f4c-486e-ab4a-471364f800d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cornell_movie_dialog import CornellMovieDialog\n",
    "\n",
    "from datasets import Split\n",
    "cornell_movie_dialog = CornellMovieDialog(data_dir=dataset_dir)\n",
    "cornell_movie_dialog.set_training_size(size=10000)\n",
    "cornell_movie_dialog.set_validation_size(size=100)\n",
    "\n",
    "cornell_movie_dialog.download_and_prepare(output_dir=\"cornell_movie_dialog\")\n",
    "train_dataset = cornell_movie_dialog.as_dataset(split=Split(name=\"train\"))\n",
    "validation_dataset = cornell_movie_dialog.as_dataset(split=Split(name=\"validation\"))\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b3b91-d193-4c10-a4cd-ba435af5c3f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base model(llama-2-7b-hf) and tokenizer\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.float16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False # silence the warnings. Please re-enable for inference!\n",
    "model.config.pretraining_tp = 1\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cfe572-fd83-4a38-8364-c00eed3ad115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#monitering login\n",
    "wandb.login(key=\"your wandb key\")\n",
    "run = wandb.init(project='Fine tuning llama-2-7B-chat', job_type=\"training\", anonymous=\"allow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d318219-18ce-4caa-8b76-fa21888e7a4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    lora_alpha= 8,\n",
    "    lora_dropout= 0.1,\n",
    "    r= 16,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee492929-5425-4387-bd8b-d90fa9fa5e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir= \"./results\",\n",
    "    num_train_epochs= 2,\n",
    "    per_device_train_batch_size= 4,\n",
    "    gradient_accumulation_steps= 2,\n",
    "    optim = \"paged_adamw_8bit\",\n",
    "    save_steps= 1000,\n",
    "    logging_steps= 30,\n",
    "    learning_rate= 2e-4,\n",
    "    weight_decay= 0.001,\n",
    "    fp16= False,\n",
    "    bf16= False,\n",
    "    max_grad_norm= 0.3,\n",
    "    max_steps= -1,\n",
    "    warmup_ratio= 0.3,\n",
    "    group_by_length= True,\n",
    "    lr_scheduler_type= \"linear\",\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190064ea-93fd-484c-8f17-882b272da467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting sft parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length= None,\n",
    "    dataset_text_field=\"messages\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7de55-7383-485c-baad-c94b1e53844a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "# Train model\n",
    "trainer.train()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b059c65-07b5-43fe-b320-6814af16fe60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "wandb.finish()\n",
    "model.config.use_cache = True\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e4aeea-a573-4c6f-98e1-dbbf33113ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream(user_prompt):\n",
    "    runtimeFlag = \"cuda:0\"\n",
    "    INST_O, INST_C = \"[INST]\", \"[/INST]\"\n",
    "    SYS_O, SYS_C = \"<<SYS>>\", \"<</SYS>>\"\n",
    "    \n",
    "    system_prompt = 'Act as a friend that is friendly and try to keep conversation going by asking questions about user'\n",
    "\n",
    "    prompt = f\"<s>{INST_O}{SYS_O}\\n{system_prompt}\\n{SYS_C}</s>\"\n",
    "    prompt += f\"<s>{INST_O}{user_prompt.strip()}{INST_C}</s>\"\n",
    "    print(prompt)\n",
    "    print(\"===============\")\n",
    "\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(runtimeFlag)\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
    "    _ = model.generate(**inputs, streamer=streamer, max_new_tokens=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f8c41c-b025-497b-9a25-319f83880d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stream(\"Hi, what is going on?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa054b8c-3075-45b5-991b-a3d4a04ca7e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear the memory footprint\n",
    "# del model, trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc3a87-e511-46cb-bb45-f481c79cab6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, low_cpu_mem_usage=True,\n",
    "    return_dict=True,torch_dtype=torch.float16,\n",
    "    device_map= {\"\": 0})\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model.load_adapter(new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68848b5-f9a9-4902-8432-4d6334abce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(new_model)\n",
    "tokenizer.push_to_hub(new_model)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
